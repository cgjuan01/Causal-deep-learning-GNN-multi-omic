#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
retrain_exercise_TEL_GNN.py

EXERCISE-ONLY, MULTI-OMIC + GLYCAN-PATHWAY GNN TRAINING

Omic layers used as FEATURES (all exercise-derived):
  1) Proteomic (PPP MR)
  2) Epigenomic (mQTL / DNAm)
  3) Glycomic (glycan / gQTL)
  4) Transcriptomic (bulk + single-cell eQTL together)
  5) TEL_ex (exercise TEL features)
  6) Glycan-pathway knowledge layer (binary 0/1 for key N-glycan biosynthesis enzymes)

This script implements TWO modes:

  (A) SELF-SUPERVISED / CONTRASTIVE GNN (NO LABELS)
      - Always runs.
      - Learns exercise-driven embeddings by contrasting two augmented views.
      - Produces:
          GNN_exercise_score_ssl
          GNN_exercise_rank_ssl

  (B) SUPERVISED GNN (OPTIONAL)
      - Only runs if you supply --label_col <name>.
      - Uses that column as a generic binary label (0/1) for evaluation.
      - Produces:
          GNN_exercise_score_sup
          GNN_exercise_rank_sup
      - Prints comparison between SSL+linear probe vs supervised GNN.

Important:
  - NO use of 'hm_total' or any hard-coded ageing label.
  - NO ageing clocks, NO PC1, NO 'age' tokens in features.
  - NO Hallmarks columns (hm_*) as inputs.
  - NO previous GNN_* columns as inputs.

File paths:
  Nodes IN:
    /Users/ciara/Downloads/LDaware_MR_proteins/protein_GNN_WITH_SCORES_EXERCISE_ONLY_TEL_FULL2940_TELrecalc.tsv

  Edges IN (optional, falls back to kNN if not present or not mappable):
    /Users/ciara/Downloads/LDaware_MR_proteins/protein_network_edges.tsv

  Nodes OUT (with new GNN ranks/scores):
    /Users/ciara/Downloads/LDaware_MR_proteins/protein_GNN_WITH_SCORES_EXERCISE_ONLY_TEL_GNNretrained.tsv
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors

import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from torch_geometric.data import Data
    from torch_geometric.nn import GCNConv
except ImportError:
    print("‚ùå torch_geometric not found. Install with:")
    print("    pip install torch-geometric torch-scatter torch-sparse")
    sys.exit(1)


# ============================================================
# 1. MODELS
# ============================================================

class GCNEncoder(nn.Module):
    """GCN encoder that outputs node embeddings."""
    def __init__(self, in_dim: int, hidden_dim: int = 64, emb_dim: int = 64, dropout: float = 0.3):
        super().__init__()
        self.conv1 = GCNConv(in_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, emb_dim)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        return x


class GCNClassifier(nn.Module):
    """GCN encoder + linear classifier head for supervised mode."""
    def __init__(self, in_dim: int, hidden_dim: int = 64, emb_dim: int = 64,
                 out_dim: int = 2, dropout: float = 0.3):
        super().__init__()
        self.encoder = GCNEncoder(in_dim, hidden_dim, emb_dim, dropout)
        self.classifier = nn.Linear(emb_dim, out_dim)

    def forward(self, x, edge_index):
        h = self.encoder(x, edge_index)
        out = self.classifier(h)
        return out, h


# ============================================================
# 2. UTILS
# ============================================================

def log(msg: str):
    print(msg, flush=True)


def build_feature_matrix(df: pd.DataFrame) -> (np.ndarray, list):
    """
    Build PURE EXERCISE feature matrix from:
      1) Proteomic (PPP MR)
      2) Epigenomic (mQTL / DNAm)
      3) Glycomic (glycan / gQTL)
      4) Transcriptomic (bulk + single-cell eQTL together)
      5) TEL_ex features
      6) Glycan-pathway knowledge layer (if present in df as 'glycan_pathway')

    EXCLUDES:
      - Any column whose name (lowercased) contains 'clock', 'age', or 'pc1'
      - Any Hallmark columns starting with 'hm_'
      - Any previous GNN/meta columns starting with 'gnn_'
      - Any non-numeric columns (strings, categories)
    """

    # ---------- 1) PROTEOMIC LAYER (PPP MR) ----------
    proteomic_core = [
        "MR_beta", "MR_se", "MR_p", "MR_FDR",
        "MR_log10", "MR_neglog", "MR_z",
        "neglog_p", "FDR_BH",
        "causal_score", "causal_rank", "causal_rank_scaled",
        "nsnp", "nsnp_scaled",
    ]

    proteomic_pattern_cols = [
        c for c in df.columns
        if any(p in c.lower() for p in [
            "proteomic_",    # e.g. proteomic_TEL_exercise
            "pqtl_",         # explicit proteomic QTL
            "protein_",      # protein_* annotations (we'll filter non-numeric later)
        ])
    ]

    proteomic_features = proteomic_core + proteomic_pattern_cols

    # ---------- 2) EPIGENOMIC LAYER (DNAm / mQTL) ----------
    epigenomic_features = [
        c for c in df.columns
        if any(p in c.lower() for p in [
            "mqtl_",         # methylation QTL
            "dnam",          # DNAm_* style
            "meth",          # methylation_*
            "cpg",           # CpG_* support
            "epigen",        # epigenomic_*
        ])
    ]

    # ---------- 3) GLYCOMIC LAYER (glycan / gQTL) ----------
    glycomic_features = [
        c for c in df.columns
        if any(p in c.lower() for p in [
            "glycan_",       # glycan_* fields
            "gqtl_",         # glycan qtl
            "glyco",         # glycomic_* etc
            "igg_",          # IgG_* if used
            "cf_",           # core fucosylation markers
        ])
    ]

    # ---------- 4) TRANSCRIPTOMIC LAYER (bulk + sc-eQTL) ----------
    transcriptomic_explicit = [
        "sc_MR_beta_mean", "sc_MR_min_p", "sc_MR_min_FDR",
        "sc_MR_support", "sc_n_celltypes",
        "sc_min_p", "sc_min_FDR",
        "gene_sc_neglog10_p", "gene_sc_neglog10_FDR",
        "gene_multiomics_support",
    ]

    transcriptomic_pattern_cols = [
        c for c in df.columns
        if any(p in c.lower() for p in [
            "eqtl_",         # bulk eQTL prefixed
            "eqtl",          # generic eQTL
            "rna_",          # RNA_* expression
            "expr_",         # expr_* features
            "expression_",   # expression_* features
            "transcriptomic_",
            "sceqtl",
            "sc_eqtl",
        ])
    ]

    transcriptomic_features = transcriptomic_explicit + transcriptomic_pattern_cols

    # ---------- TEL-LIKE EXERCISE FEATURES (cross-omic) ----------
    tel_features = [
        c for c in df.columns
        if ("tel" in c.lower())
        and ("clock" not in c.lower())
        and ("age" not in c.lower())
        and ("pc1" not in c.lower())
    ]

    # ---------- GLYCAN-PATHWAY KNOWLEDGE LAYER ----------
    knowledge_features = []
    if "glycan_pathway" in df.columns:
        knowledge_features.append("glycan_pathway")

    # ---------- CONCATENATE ----------
    candidate_features = (
        proteomic_features +
        epigenomic_features +
        glycomic_features +
        transcriptomic_features +
        tel_features +
        knowledge_features
    )

    # De-duplicate while preserving order and keep only existing columns
    seen = set()
    all_features = []
    for c in candidate_features:
        if c in df.columns and c not in seen:
            seen.add(c)
            all_features.append(c)

    # ---------- FILTER OUT HALLMARKS, OLD GNN, CLOCK/AGE/PC1 ----------
    forbidden_tokens = ["pc1", "clock", "age"]
    cleaned_features = []
    for c in all_features:
        cl = c.lower()
        if cl.startswith("hm_"):         # drop all Hallmarks columns
            continue
        if cl.startswith("gnn_"):        # drop any previous GNN_* scores
            continue
        if any(tok in cl for tok in forbidden_tokens):
            continue
        cleaned_features.append(c)

    # ---------- KEEP ONLY NUMERIC COLUMNS ----------
    numeric_features = [
        c for c in cleaned_features
        if pd.api.types.is_numeric_dtype(df[c])
    ]

    if not numeric_features:
        raise ValueError(
            "No valid numeric exercise multi-omic features found! "
            "Check column names and dtypes in your node table."
        )

    log(
        f"‚úÖ Using {len(numeric_features)} PURE EXERCISE multi-omic NUMERIC feature columns "
        f"(4 omics + TEL + glycan-pathway knowledge):"
    )
    for c in numeric_features:
        log(f"   - {c}")

    X = df[numeric_features].fillna(0.0).values.astype(np.float32)
    return X, numeric_features


def build_edge_index_from_file(df: pd.DataFrame, edge_path: str):
    """
    Build edge_index from an external edge file if possible.
    Edge file should have columns like ['source', 'target'] using gene_symbol.
    """
    if not os.path.exists(edge_path):
        log(f"‚ö†Ô∏è Edge file not found at: {edge_path}")
        return None

    log(f"üìÇ Loading edge list from:\n   {edge_path}")
    edges = pd.read_csv(edge_path, sep="\t")

    # Try a few common column names
    possible_src = [c for c in edges.columns if c.lower() in ("source", "from", "gene_a", "node1")]
    possible_tgt = [c for c in edges.columns if c.lower() in ("target", "to", "gene_b", "node2")]

    if not possible_src or not possible_tgt:
        log("‚ö†Ô∏è Could not detect source/target columns in edge file. Falling back to kNN graph.")
        return None

    src_col = possible_src[0]
    tgt_col = possible_tgt[0]

    if "gene_symbol" not in df.columns:
        log("‚ö†Ô∏è 'gene_symbol' not in node table. Cannot map edges. Falling back to kNN graph.")
        return None

    # Map gene_symbol -> index
    name_to_idx = {g: i for i, g in enumerate(df["gene_symbol"].astype(str).values)}

    src_idx = []
    tgt_idx = []
    n_skipped = 0

    for _, row in edges.iterrows():
        s = str(row[src_col])
        t = str(row[tgt_col])
        if s in name_to_idx and t in name_to_idx:
            src_idx.append(name_to_idx[s])
            tgt_idx.append(name_to_idx[t])
        else:
            n_skipped += 1

    if not src_idx:
        log("‚ö†Ô∏è No edges could be mapped to node table. Falling back to kNN graph.")
        return None

    edge_index = torch.tensor([src_idx, tgt_idx], dtype=torch.long)
    log(
        f"‚úÖ Built edge_index from file with {edge_index.size(1)} edges "
        f"(skipped {n_skipped} edges that didn't match)."
    )
    return edge_index


def build_edge_index_knn(X: np.ndarray, k: int = 10) -> torch.Tensor:
    """
    Build a simple kNN graph from feature space if no edge file is available.
    """
    log(f"üìê Building kNN graph with k={k} from exercise features.")
    nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm="auto").fit(X)
    distances, indices = nbrs.kneighbors(X)

    src = []
    tgt = []
    n_nodes = X.shape[0]
    for i in range(n_nodes):
        # indices[i][0] is i itself; skip it
        for j in indices[i][1:]:
            src.append(i)
            tgt.append(j)

    edge_index = torch.tensor([src, tgt], dtype=torch.long)
    log(f"‚úÖ kNN graph built with {edge_index.size(1)} edges.")
    return edge_index


def make_masks(n_nodes: int, y: np.ndarray, train_frac=0.7, val_frac=0.15, seed=42):
    """
    Create boolean masks for train/val/test (node classification).
    """
    idx = np.arange(n_nodes)
    idx_train, idx_temp, y_train, y_temp = train_test_split(
        idx, y, train_size=train_frac, stratify=y, random_state=seed
    )
    val_size = val_frac / (1.0 - train_frac)
    idx_val, idx_test, _, _ = train_test_split(
        idx_temp, y_temp, train_size=val_size, stratify=y_temp, random_state=seed
    )

    train_mask = torch.zeros(n_nodes, dtype=torch.bool)
    val_mask = torch.zeros(n_nodes, dtype=torch.bool)
    test_mask = torch.zeros(n_nodes, dtype=torch.bool)

    train_mask[idx_train] = True
    val_mask[idx_val] = True
    test_mask[idx_test] = True

    log(
        f"üß™ Train/Val/Test sizes: "
        f"{train_mask.sum().item()}/{val_mask.sum().item()}/{test_mask.sum().item()}"
    )
    return train_mask, val_mask, test_mask


# ============================================================
# 3. SELF-SUPERVISED CONTRASTIVE TRAINING
# ============================================================

def feature_dropout(x: torch.Tensor, drop_prob: float = 0.2) -> torch.Tensor:
    """Randomly drop features (node-wise) for contrastive augmentations."""
    if drop_prob <= 0.0:
        return x
    mask = (torch.rand_like(x) > drop_prob).float()
    return x * mask


def nt_xent_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.5) -> torch.Tensor:
    """
    NT-Xent contrastive loss (SimCLR-style) for two views of the same batch.
    """
    N = z1.size(0)
    z1 = F.normalize(z1, p=2, dim=1)
    z2 = F.normalize(z2, p=2, dim=1)
    z = torch.cat([z1, z2], dim=0)         # [2N, d]

    sim = torch.matmul(z, z.T) / temperature  # [2N, 2N]
    # mask out self-similarity
    mask = torch.eye(2 * N, dtype=torch.bool, device=z.device)
    sim = sim.masked_fill(mask, -9e15)

    # positive indices: for i in [0..N-1], positive is i+N; for i in [N..2N-1], positive is i-N
    pos_indices = torch.cat([
        torch.arange(N, 2 * N, device=z.device),
        torch.arange(0, N, device=z.device)
    ])
    pos_sim = sim[torch.arange(2 * N, device=z.device), pos_indices]  # [2N]

    # denominator: all similarities for each anchor
    logsumexp = torch.logsumexp(sim, dim=1)  # [2N]

    loss = -pos_sim + logsumexp
    return loss.mean()


def train_gnn_ssl(data: Data, in_dim: int, emb_dim: int = 64,
                  n_epochs: int = 150, lr: float = 1e-3,
                  weight_decay: float = 5e-4, device="cpu"):
    """
    Self-supervised contrastive training.
    """
    encoder = GCNEncoder(in_dim=in_dim, hidden_dim=64, emb_dim=emb_dim, dropout=0.3).to(device)
    projector = nn.Sequential(
        nn.Linear(emb_dim, emb_dim),
        nn.ReLU(),
        nn.Linear(emb_dim, emb_dim)
    ).to(device)

    data = data.to(device)

    optimizer = torch.optim.Adam(
        list(encoder.parameters()) + list(projector.parameters()),
        lr=lr,
        weight_decay=weight_decay
    )

    for epoch in range(1, n_epochs + 1):
        encoder.train()
        projector.train()
        optimizer.zero_grad()

        # Two feature-dropout views
        x1 = feature_dropout(data.x, drop_prob=0.2)
        x2 = feature_dropout(data.x, drop_prob=0.2)

        h1 = encoder(x1, data.edge_index)
        h2 = encoder(x2, data.edge_index)

        z1 = projector(h1)
        z2 = projector(h2)

        loss = nt_xent_loss(z1, z2, temperature=0.5)
        loss.backward()
        optimizer.step()

        if epoch % 20 == 0 or epoch == 1:
            log(f"[SSL] Epoch {epoch:03d} | loss={loss.item():.4f}")

    # Final embeddings from encoder (no projector)
    encoder.eval()
    with torch.no_grad():
        emb = encoder(data.x, data.edge_index).cpu()

    return encoder, emb


# ============================================================
# 4. SUPERVISED TRAINING (OPTIONAL)
# ============================================================

def train_gnn_supervised(data: Data, in_dim: int, emb_dim: int = 64,
                         n_epochs: int = 150, lr: float = 1e-3,
                         weight_decay: float = 5e-4, device="cpu"):
    """
    Supervised node classification, if labels are provided.
    """
    model = GCNClassifier(in_dim=in_dim, hidden_dim=64, emb_dim=emb_dim,
                          out_dim=2, dropout=0.3).to(device)
    data = data.to(device)

    # Class weights for imbalance
    y_np = data.y.cpu().numpy()
    classes, counts = np.unique(y_np, return_counts=True)
    class_weights = np.zeros(len(classes), dtype=np.float32)
    for c, cnt in zip(classes, counts):
        class_weights[c] = 1.0 / max(cnt, 1)
    class_weights = class_weights / class_weights.sum() * len(classes)
    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)

    log(f"[SUP] Class distribution: {dict(zip(classes.tolist(), counts.tolist()))}")
    log(f"[SUP] Class weights: {class_weights.detach().cpu().numpy().tolist()}")

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    best_val_loss = float("inf")
    best_state = None

    for epoch in range(1, n_epochs + 1):
        model.train()
        optimizer.zero_grad()

        out, _ = model(data.x, data.edge_index)
        loss = criterion(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()

        # Eval
        model.eval()
        with torch.no_grad():
            logits, _ = model(data.x, data.edge_index)
            val_loss = criterion(logits[data.val_mask], data.y[data.val_mask])

            pred = logits.argmax(dim=1)
            train_acc = (pred[data.train_mask] == data.y[data.train_mask]).float().mean().item()
            val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean().item()

        if val_loss.item() < best_val_loss:
            best_val_loss = val_loss.item()
            best_state = model.state_dict()

        if epoch % 20 == 0 or epoch == 1:
            log(
                f"[SUP] Epoch {epoch:03d} | "
                f"train_loss={loss.item():.4f} | "
                f"val_loss={val_loss.item():.4f} | "
                f"train_acc={train_acc:.3f} | "
                f"val_acc={val_acc:.3f}"
            )

    if best_state is not None:
        model.load_state_dict(best_state)
        log(f"[SUP] Loaded best model (val_loss={best_val_loss:.4f}).")

    # Final test performance
    model.eval()
    with torch.no_grad():
        logits, emb = model(data.x, data.edge_index)
        test_pred = logits.argmax(dim=1)
        test_acc = (test_pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()
    log(f"[SUP] Final test accuracy: {test_acc:.3f}")

    return model, emb.cpu(), logits.cpu(), test_acc


# ============================================================
# 5. MAIN
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="Retrain PURE EXERCISE-ONLY, MULTI-OMIC + GLYCAN-PATHWAY TEL GNN "
                    "(SSL + optional SUP, layer-weighted)"
    )
    parser.add_argument(
        "--nodes",
        default="/Users/ciara/Downloads/LDaware_MR_proteins/protein_GNN_WITH_SCORES_EXERCISE_ONLY_TEL_FULL2940_TELrecalc.tsv",
        help="Path to node table TSV."
    )
    parser.add_argument(
        "--edges",
        default="/Users/ciara/Downloads/LDaware_MR_proteins/protein_network_edges.tsv",
        help="Path to edge list TSV (optional)."
    )
    parser.add_argument(
        "--out",
        default="/Users/ciara/Downloads/LDaware_MR_proteins/protein_GNN_WITH_SCORES_EXERCISE_ONLY_TEL_GNNretrained.tsv",
        help="Output TSV with GNN scores/ranks."
    )
    parser.add_argument(
        "--label_col",
        default=None,
        help="Optional label column name for supervised comparison (e.g. a binary 0/1 column)."
    )
    parser.add_argument("--epochs_ssl", type=int, default=150, help="Number of SSL training epochs.")
    parser.add_argument("--epochs_sup", type=int, default=150, help="Number of supervised training epochs.")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate.")
    parser.add_argument("--weight_decay", type=float, default=5e-4, help="Weight decay.")
    parser.add_argument("--device", default="cpu", help="Device: 'cpu' or 'cuda'.")
    args = parser.parse_args()

    node_path = args.nodes
    edge_path = args.edges
    out_path = args.out
    label_col = args.label_col

    # ----------------------------------------
    # Load node table
    # ----------------------------------------
    if not os.path.exists(node_path):
        log(f"‚ùå Node table not found at:\n   {node_path}")
        sys.exit(1)

    log("üìÇ Loading node table:")
    log(f"   {node_path}")
    df = pd.read_csv(node_path, sep="\t")
    log(f"‚úÖ Loaded {df.shape[0]} nodes and {df.shape[1]} columns.")

    # ----------------------------------------
    # Add GLYCAN-PATHWAY knowledge layer (binary)
    # ----------------------------------------
    glycan_pathway_genes = [
        "FUT8", "ST3GAL1", "ST6GAL1", "MAN1A2",
        "B4GALT1", "B4GALT2", "B4GALT3"
    ]

    if "gene_symbol" in df.columns:
        df["glycan_pathway"] = df["gene_symbol"].astype(str).isin(glycan_pathway_genes).astype(float)
        log(
            "üß¨ Added glycan_pathway knowledge layer (0/1) for genes: "
            + ", ".join(glycan_pathway_genes)
        )
        log(
            f"   Number of genes in glycan_pathway=1: "
            f"{int(df['glycan_pathway'].sum())}"
        )
    else:
        df["glycan_pathway"] = 0.0
        log("‚ö†Ô∏è 'gene_symbol' not in node table; glycan_pathway set to 0 for all nodes.")

    # ----------------------------------------
    # Build features (PURE EXERCISE, 4 omics + TEL + glycan-pathway)
    # ----------------------------------------
    X, feature_cols = build_feature_matrix(df)

    # Standardise features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # ----------------------------------------
    # Layer-specific weighting: upweight DNAm / glycan / sc-eQTL, keep TEL & knowledge moderate
    # ----------------------------------------
    layer_weights = {
        "proteomic": 1.0,
        "epigenomic": 3.0,
        "glycomic": 3.0,
        "transcriptomic": 3.0,
        "tel": 1.0,
        "knowledge": 2.0,  # glycan_pathway
    }

    epigen_idx = [
        i for i, c in enumerate(feature_cols)
        if any(p in c.lower() for p in ["mqtl_", "dnam", "meth", "cpg", "epigen"])
    ]
    glyco_idx = [
        i for i, c in enumerate(feature_cols)
        if any(p in c.lower() for p in ["glycan_", "gqtl_", "glyco", "igg_", "cf_"])
    ]
    trans_idx = [
        i for i, c in enumerate(feature_cols)
        if any(p in c.lower() for p in ["sc_mr_", "sceqtl", "sc_eqtl", "rna_", "expr_", "expression_", "transcriptomic_"])
        or c in [
            "sc_MR_beta_mean", "sc_MR_min_p", "sc_MR_min_FDR",
            "sc_MR_support", "sc_n_celltypes",
            "sc_min_p", "sc_min_FDR",
            "gene_sc_neglog10_p", "gene_sc_neglog10_FDR",
            "gene_multiomics_support",
        ]
    ]
    tel_idx = [
        i for i, c in enumerate(feature_cols)
        if ("tel" in c.lower()) and ("clock" not in c.lower()) and ("age" not in c.lower()) and ("pc1" not in c.lower())
    ]
    knowledge_idx = [
        i for i, c in enumerate(feature_cols)
        if c == "glycan_pathway"
    ]

    log("\nüîß Layer weighting (post-standardisation):")
    log(f"   Epigenomic features (x{layer_weights['epigenomic']}): {len(epigen_idx)}")
    log(f"   Glycomic   features (x{layer_weights['glycomic']}): {len(glyco_idx)}")
    log(f"   Transcript features (x{layer_weights['transcriptomic']}): {len(trans_idx)}")
    log(f"   TEL_ex      features (x{layer_weights['tel']}): {len(tel_idx)}")
    log(f"   Knowledge   features (x{layer_weights['knowledge']}): {len(knowledge_idx)}")

    if epigen_idx:
        X_scaled[:, epigen_idx] *= layer_weights["epigenomic"]
    if glyco_idx:
        X_scaled[:, glyco_idx] *= layer_weights["glycomic"]
    if trans_idx:
        X_scaled[:, trans_idx] *= layer_weights["transcriptomic"]
    if tel_idx:
        X_scaled[:, tel_idx] *= layer_weights["tel"]
    if knowledge_idx:
        X_scaled[:, knowledge_idx] *= layer_weights["knowledge"]

    # ----------------------------------------
    # Build edge_index
    # ----------------------------------------
    edge_index = build_edge_index_from_file(df, edge_path)
    if edge_index is None:
        edge_index = build_edge_index_knn(X_scaled, k=10)

    # ----------------------------------------
    # Build PyG Data object for SSL
    # ----------------------------------------
    x_tensor = torch.tensor(X_scaled, dtype=torch.float32)
    data_ssl = Data(x=x_tensor, edge_index=edge_index)

    device = args.device if (args.device == "cuda" and torch.cuda.is_available()) else "cpu"
    if device == "cuda":
        log("üöÄ Using CUDA")
    else:
        log("üß† Using CPU")

    in_dim = x_tensor.size(1)
    emb_dim = 64

    # ----------------------------------------
    # SELF-SUPERVISED TRAINING (ALWAYS)
    # ----------------------------------------
    log("\n===== SELF-SUPERVISED CONTRASTIVE GNN (NO LABELS, LAYER-WEIGHTED + GLYCAN-PATHWAY) =====")
    encoder_ssl, emb_ssl = train_gnn_ssl(
        data=data_ssl,
        in_dim=in_dim,
        emb_dim=emb_dim,
        n_epochs=args.epochs_ssl,
        lr=args.lr,
        weight_decay=args.weight_decay,
        device=device,
    )

    # Derive a scalar exercise score from SSL embeddings (e.g. embedding norm)
    norms = emb_ssl.norm(dim=1).numpy()
    if norms.max() > norms.min():
        scores_ssl = (norms - norms.min()) / (norms.max() - norms.min())
    else:
        scores_ssl = np.zeros_like(norms)

    df["GNN_exercise_score_ssl"] = scores_ssl
    df["GNN_exercise_rank_ssl"] = df["GNN_exercise_score_ssl"].rank(
        ascending=False, method="min"
    ).astype(int)

    log("‚úÖ Added SSL columns:")
    log("   - GNN_exercise_score_ssl")
    log("   - GNN_exercise_rank_ssl")

    # ----------------------------------------
    # OPTIONAL: SUPERVISED TRAINING + COMPARISON
    # ----------------------------------------
    if label_col is not None and label_col in df.columns:
        log(f"\n===== SUPERVISED GNN USING LABEL COLUMN: {label_col} =====")

        # Prepare labels
        y_raw = df[label_col].values
        # Basic binarisation safeguard if labels are not integers
        if y_raw.dtype.kind in {"f", "O"}:
            # threshold at median > 0, or any non-zero becomes 1
            if np.unique(y_raw).size > 2:
                median_val = np.median(y_raw)
                y = (y_raw > median_val).astype(int)
            else:
                y = (y_raw > 0).astype(int)
        else:
            y = y_raw.astype(int)

        y_tensor = torch.tensor(y, dtype=torch.long)
        train_mask, val_mask, test_mask = make_masks(n_nodes=x_tensor.size(0), y=y)

        data_sup = Data(
            x=x_tensor,
            edge_index=edge_index,
            y=y_tensor,
            train_mask=train_mask,
            val_mask=val_mask,
            test_mask=test_mask,
        )

        # Supervised GNN
        model_sup, emb_sup, logits_sup, sup_test_acc = train_gnn_supervised(
            data=data_sup,
            in_dim=in_dim,
            emb_dim=emb_dim,
            n_epochs=args.epochs_sup,
            lr=args.lr,
            weight_decay=args.weight_decay,
            device=device,
        )

        # Supervised probabilities as scores
        probs_sup = F.softmax(logits_sup, dim=1)[:, 1].numpy()
        df["GNN_exercise_score_sup"] = probs_sup
        df["GNN_exercise_rank_sup"] = df["GNN_exercise_score_sup"].rank(
            ascending=False, method="min"
        ).astype(int)

        log("‚úÖ Added supervised columns:")
        log("   - GNN_exercise_score_sup")
        log("   - GNN_exercise_rank_sup")

        # Comparison: linear probe on SSL embeddings vs supervised GNN
        log("\n===== COMPARISON: SSL + LINEAR PROBE vs SUPERVISED GNN =====")
        emb_np = emb_ssl.numpy()
        train_idx = train_mask.numpy()
        test_idx = test_mask.numpy()

        clf = LogisticRegression(max_iter=500, class_weight="balanced")
        clf.fit(emb_np[train_idx], y[train_idx])
        ssl_test_acc = clf.score(emb_np[test_idx], y[test_idx])

        log(f"[SSL+Linear] Test accuracy: {ssl_test_acc:.3f}")
        log(f"[SUP-GNN   ] Test accuracy: {sup_test_acc:.3f}")

    elif label_col is not None:
        log(f"‚ö†Ô∏è Supplied label_col '{label_col}' not found in node table. Skipping supervised mode.")

    # ----------------------------------------
    # Save output
    # ----------------------------------------
    out_dir = os.path.dirname(out_path)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)

    df.to_csv(out_path, sep="\t", index=False)
    log(f"\nüíæ Saved GNN-updated node table to:\n   {out_path}")
    log("üéâ PURE EXERCISE-ONLY, MULTI-OMIC + GLYCAN-PATHWAY TEL GNN retraining complete "
        "(SSL always; optional supervised comparison; DNAm/glycan/sc-eQTL upweighted; glycan-pathway layer added).")


if __name__ == "__main__":
    main()

